{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "s4fxc0-bH8X9",
        "Yl0aaRa3RIGP",
        "4ZcIjeMdReoQ",
        "EMFhqqUbVigH"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# How LangChain Works"
      ],
      "metadata": {
        "id": "s4fxc0-bH8X9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6dgqs8g1HRrU",
        "outputId": "fd478d7d-49d9-45c5-bb21-69e5a13221ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.11.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from groq)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.23.4)\n",
            "Downloading groq-0.11.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, groq\n",
            "Successfully installed groq-0.11.0 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2\n"
          ]
        }
      ],
      "source": [
        "!pip install groq"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up GROQ_API_KEY and Initialize the Groq client\n",
        "\n",
        "**GROQ API key:** https://console.groq.com/keys\n"
      ],
      "metadata": {
        "id": "LbDZbXIbJYPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from groq import Groq\n",
        "os.environ[\"GROQ_API_KEY\"] = \"Place Your API Key Here\"\n",
        "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
        "print(\"Initialized Groq client.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQqv1AkKJUgf",
        "outputId": "6ceec612-9da4-458e-9792-a19c5c60c3b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized Groq client.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: User Input – This is where everything starts.\n",
        "The user provides a query"
      ],
      "metadata": {
        "id": "jEA6e3HoJzSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"What is LangChain?\"\n",
        "print(f\"User Input: {user_query}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHDC0a6SJoGF",
        "outputId": "025c7970-1e2a-4464-e0fe-b52589cb1d42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User Input: What is LangChain?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Preprocessing Chain – This prepares and transforms the user's input before passing it to the LLM.\n",
        "\n",
        "In a more complex application, this could involve more extensive processing like tokenization, input validation, and formatting.\n",
        "For this example, I am doing only simply strip any extra whitespace."
      ],
      "metadata": {
        "id": "Y4mlLACdJ-tA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_query = user_query.strip()\n",
        "print(f\"Preprocessed Query: {preprocessed_query}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q31UXBoXJyq9",
        "outputId": "942a2d40-e8bb-483a-c29e-e4a7a5cb8caa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed Query: What is LangChain?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: LLM Query – The language model processes the question and searches for a relevant answer.\n",
        "Create a chat completion request to the language model"
      ],
      "metadata": {
        "id": "FO5047lVKepA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": preprocessed_query,\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama3-8b-8192\",\n",
        ")\n",
        "print(\"Query sent to LLM.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rm4vrN2RKNfM",
        "outputId": "f1634575-2672-4760-acc5-9fe6ce298f43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query sent to LLM.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Postprocessing Chain – The answer is formatted by using methods such as filtering, summarization, and answer enhancement.\n",
        "\n",
        "Extract and enhance the response from the LLM"
      ],
      "metadata": {
        "id": "JHShgLsFKpbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat_completion.choices[0].message.content\n",
        "print(\"Response received from LLM.\")\n",
        "enhanced_response = response.strip()\n",
        "print(f\"Enhanced Response: {enhanced_response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsKOJtzlKmxI",
        "outputId": "b5aaab1a-910c-4d84-c0e3-d724ed9f4dc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response received from LLM.\n",
            "Enhanced Response: LangChain is a new AI language model that has gained significant attention in recent times. LangChain is a large-scale transformer-based language model developed by Meta AI, a leading AI research organization. It is designed to process and generate human-like language, with a focus on understanding and generating coherent and context-dependent text.\n",
            "\n",
            "Some of the key features and capabilities of LangChain include:\n",
            "\n",
            "1. **Scale**: LangChain has been trained on an enormous corpus of text data, making it one of the largest language models currently available. This scale allows it to learn complex patterns and relationships in language.\n",
            "2. **Contextual understanding**: LangChain is designed to understand the context in which a piece of text is written, including the relationship between sentences, paragraphs, and the broader topic or theme.\n",
            "3. **Generative capabilities**: LangChain can generate coherent and engaging text on a wide range of topics, from short phrases to longer documents. It can also be fine-tuned for specific tasks, such as language translation, text summarization, and chatbot applications.\n",
            "4. **Multi-modal capabilities**: LangChain can process and generate text, images, and video, making it a versatile AI model for various applications, such as natural language processing, computer vision, and multimedia processing.\n",
            "\n",
            "Overall, LangChain is an exciting development in the field of AI research, with the potential to revolutionize the way we interact with language and generate text. Its capabilities have many potential applications, such as:\n",
            "\n",
            "* Natural language interfaces for AI systems\n",
            "* Content generation for various industries, such as marketing, entertainment, and publishing\n",
            "* Improved language translation and summarization\n",
            "* Chatbots and conversational AI systems\n",
            "\n",
            "Keep in mind that LangChain is still a research model, and its capabilities may evolve as it is further developed and refined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Output – The final result is presented to the user."
      ],
      "metadata": {
        "id": "hWzhFyvEK6r_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Final Output:\")\n",
        "print(enhanced_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2G2EaSfOKvnY",
        "outputId": "88839215-abb3-473d-eb70-0f7e3756b23e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Output:\n",
            "LangChain is a new AI language model that has gained significant attention in recent times. LangChain is a large-scale transformer-based language model developed by Meta AI, a leading AI research organization. It is designed to process and generate human-like language, with a focus on understanding and generating coherent and context-dependent text.\n",
            "\n",
            "Some of the key features and capabilities of LangChain include:\n",
            "\n",
            "1. **Scale**: LangChain has been trained on an enormous corpus of text data, making it one of the largest language models currently available. This scale allows it to learn complex patterns and relationships in language.\n",
            "2. **Contextual understanding**: LangChain is designed to understand the context in which a piece of text is written, including the relationship between sentences, paragraphs, and the broader topic or theme.\n",
            "3. **Generative capabilities**: LangChain can generate coherent and engaging text on a wide range of topics, from short phrases to longer documents. It can also be fine-tuned for specific tasks, such as language translation, text summarization, and chatbot applications.\n",
            "4. **Multi-modal capabilities**: LangChain can process and generate text, images, and video, making it a versatile AI model for various applications, such as natural language processing, computer vision, and multimedia processing.\n",
            "\n",
            "Overall, LangChain is an exciting development in the field of AI research, with the potential to revolutionize the way we interact with language and generate text. Its capabilities have many potential applications, such as:\n",
            "\n",
            "* Natural language interfaces for AI systems\n",
            "* Content generation for various industries, such as marketing, entertainment, and publishing\n",
            "* Improved language translation and summarization\n",
            "* Chatbots and conversational AI systems\n",
            "\n",
            "Keep in mind that LangChain is still a research model, and its capabilities may evolve as it is further developed and refined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain Components"
      ],
      "metadata": {
        "id": "Yl0aaRa3RIGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfBZWdSmRgQ3",
        "outputId": "4042b107-78b2-447b-ad85-ea43b5b94d76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq) (0.27.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.23.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up GROQ_API_KEY and Initialize the Groq client\n",
        "\n",
        "\n",
        "**GROQ API key:** https://console.groq.com/keys\n"
      ],
      "metadata": {
        "id": "dxN6cJBWRmXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from groq import Groq\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = \"Place Your API Key Here\"\n",
        "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
        "print(\"Initialized Groq client with API key.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmzSjrvKRHe0",
        "outputId": "093456b4-0154-41b8-9879-50f74cea0ba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized Groq client with API key.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. LLMs - Text Generation"
      ],
      "metadata": {
        "id": "L5GQkb9FRy7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# User-defined theme for the story\n",
        "theme = \"A Student learning the basics of LangChain\"\n",
        "print(f\"User-defined theme: {theme}\")\n",
        "\n",
        "# Generate story using LLM\n",
        "story_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Generate a creative story about {theme}.\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama3-8b-8192\",\n",
        ")\n",
        "print(\"Story generation request sent to the LLM.\")\n",
        "\n",
        "# Print the generated story\n",
        "print(\"Generated Story:\")\n",
        "print(story_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzFgFP_1K4u3",
        "outputId": "493b71d1-4d77-4a4c-9ed4-e11ab31524bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User-defined theme: A Student learning the basics of LangChain\n",
            "Story generation request sent to the LLM.\n",
            "Generated Story:\n",
            "**The Mysterious Box of LangChain**\n",
            "\n",
            "It was a typical Friday morning for Emily, a student in her second year of college, huddled in the campus library's corner, surrounded by stacks of books and notes. She had spent the entire week stressing about the upcoming exams, and her professor, Dr. Patel, had just assigned a challenging project that required mastering the basics of LangChain, a revolutionary natural language processing (NLP) framework.\n",
            "\n",
            "As Emily scrolled through her email, her eyes widened at the sight of an unmarked package with her name on it. Curious, she ripped open the box to find a sleek, futuristic device called the \"LangChain Primer.\"\n",
            "\n",
            "The device, about the size of a smartwatch, glowed with an eerie blue light. Suddenly, a holographic interface materialized before her, beckoning her to touch the screen. Emily hesitated for a moment before gingerly placing her finger on the virtual surface.\n",
            "\n",
            "The interface opened to a playful, interactive tutorial, introducing Emily to the world of LangChain. The first challenge was to define a simple text processing task: breaking down a long paragraph into individual sentences. Emily was amazed as the device seamlessly guided her through the process, using visualizations and explanations to help her understand the intricacies of LangChain's architecture.\n",
            "\n",
            "As she progressed through the tutorial, the challenges grew more complex. Emily was asked to identify entities, sentiments, and intentions within a set of texts. The device expertly highlighted critical concepts, such as tokenization, part-of-speech tagging, and named entity recognition.\n",
            "\n",
            "But the turning point came when Emily encountered a mysterious task: generating a response to a simple question using LangChain's natural language generation (NLG) capabilities. The device provided a prompt: \"What is the best way to spend a Saturday afternoon?\"\n",
            "\n",
            "With a mix of excitement and trepidation, Emily input her response using a combination of pre-existing knowledge and her own creative flair. To her astonishment, the device transformed her words into a coherent, human-like response: \"A Saturday afternoon is the perfect time to relax with a good book, take a leisurely walk in the park, or try that new recipe you've been meaning to try.\"\n",
            "\n",
            "The tutorial concluded, and Emily couldn't help but feel a sense of accomplishment and wonder. She had unlocked the door to a vast, hidden world of language and possibilities. As she looked around the library, she noticed students engaged in lively discussions, oblivious to the LangChain device silently assisting them in their own learning journey.\n",
            "\n",
            "Emily returned to her dorm, determined to explore the full potential of LangChain. Over the next few days, she spent countless hours honing her skills, collaborating with fellow students, and even creating her own innovative LangChain-based projects.\n",
            "\n",
            "As the project deadline approached, Emily submitted her masterpiece: a LangChain-powered chatbot that helped fellow students overcome common language barriers. Dr. Patel beamed with pride, praising Emily's creativity and technical mastery.\n",
            "\n",
            "The mysterious box of LangChain had become a catalyst for Emily's own transformation, sparking a passion for NLP and a newfound appreciation for the art of language. And as she gazed at the device, now a symbol of her own growth and discovery, she knew that the possibilities were endless, bound only by the limits of her imagination.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Prompts - Simplifying Information"
      ],
      "metadata": {
        "id": "5IxM9T4GSFUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a complex topic to simplify\n",
        "complex_topic = \"Explain the concept of LangChain\"\n",
        "print(f\"Complex topic to simplify: {complex_topic}\")\n",
        "\n",
        "# Get simplified explanation\n",
        "simple_explanation = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Explain '{complex_topic}' like I'm 5.\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama3-8b-8192\",\n",
        ")\n",
        "print(\"Request for simplified explanation sent to the LLM.\")\n",
        "\n",
        "# Print the simplified explanation\n",
        "print(\"Simplified Explanation:\")\n",
        "print(simple_explanation.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQiex1DQSAjH",
        "outputId": "b32b0189-3020-4806-be16-19aa7f7d3274"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complex topic to simplify: Explain the concept of LangChain\n",
            "Request for simplified explanation sent to the LLM.\n",
            "Simplified Explanation:\n",
            "Oh boy, are you going to love this!\n",
            "\n",
            "So, you know how we can have conversations, like you and I right now? We talk and talk and understand each other.\n",
            "\n",
            "Well, LangChain is like a super smart computer friend that can have conversations with us too! But instead of being a person, it's a special kind of computer program called a \"language model\".\n",
            "\n",
            "Imagine you have a toy box full of blocks, and each block has a letter or a word on it. When you put the blocks together in a special order, you can make a sentence or a paragraph. That's kind of like what LangChain does, but instead of blocks, it uses special computer code to understand and create words and sentences.\n",
            "\n",
            "LangChain is super good at talking and understanding us because it was trained on a huge amount of information, like a giant bookshelf full of books! It read and learned from all those books, so it can recognize words, patterns, and even emotions. \n",
            "\n",
            "Now, when you talk to LangChain, it listens to what you say and responds back in a way that makes sense. It's like having a conversation with a really smart friend who knows a lot of things! \n",
            "\n",
            "But here's the amazing part: LangChain can also generate its own sentences and ideas, just like a human would. It's like you're having a conversation, and then LangChain starts saying things that are even more interesting and creative than what you were saying!\n",
            "\n",
            "So, LangChain is like a super smart, super helpful, and super fun computer friend that can talk and create with us!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Chains - Data Processing Workflow"
      ],
      "metadata": {
        "id": "Rb_ygjIrSkbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate data fetching from a database\n",
        "data = \"Here are the sales numbers for Q1: $1000, Q2: $1500\"\n",
        "print(\"Fetched data for report generation.\")\n",
        "print(f\"Data: {data}\")\n",
        "\n",
        "# Generate a report based on fetched data\n",
        "report_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Based on the following data, generate a sales report: {data}\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama3-8b-8192\",\n",
        ")\n",
        "print(\"Request for sales report sent to the LLM.\")\n",
        "\n",
        "# Print the generated report\n",
        "print(\"Generated Sales Report:\")\n",
        "print(report_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPBVdlUaSeaZ",
        "outputId": "26650f4f-1c10-4441-dd38-72ae3ed31e13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetched data for report generation.\n",
            "Data: Here are the sales numbers for Q1: $1000, Q2: $1500\n",
            "Request for sales report sent to the LLM.\n",
            "Generated Sales Report:\n",
            "Here is a sales report based on the provided data:\n",
            "\n",
            "**Sales Report for Q1 and Q2**\n",
            "\n",
            "**Quarterly Sales Summary**\n",
            "\n",
            "| Quarter | Sales |\n",
            "| --- | --- |\n",
            "| Q1 | $1000 |\n",
            "| Q2 | $1500 |\n",
            "\n",
            "**Total Sales**\n",
            "\n",
            "* Q1: $1000\n",
            "* Q2: $1500\n",
            "* Total: $2500\n",
            "\n",
            "**Quarter-over-Quarter Comparison**\n",
            "\n",
            "* Q1 to Q2: +50% increase in sales\n",
            "\n",
            "**Key Takeaways**\n",
            "\n",
            "* Total sales for the first half of the year (Q1 and Q2) are $2500.\n",
            "* Sales have increased by 50% from Q1 to Q2.\n",
            "\n",
            "Let me know if you'd like me to add anything else to the report!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Agents - Decision-Making in Chatbots"
      ],
      "metadata": {
        "id": "KqM4qd-4StqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# User query that needs a decision\n",
        "user_query = \"I'm having trouble with my order.\"\n",
        "print(f\"User query: {user_query}\")\n",
        "\n",
        "# Simulate agent decision-making\n",
        "agent_response = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"User query: '{user_query}'. Should I respond or escalate this issue?\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama3-8b-8192\",\n",
        ")\n",
        "print(\"Request for agent decision sent to the LLM.\")\n",
        "\n",
        "# Print agent's decision\n",
        "print(\"Agent's Decision:\")\n",
        "print(agent_response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcmps2TtSqEM",
        "outputId": "54ed5bba-0b6e-4817-b281-43f8c2027d1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User query: I'm having trouble with my order.\n",
            "Request for agent decision sent to the LLM.\n",
            "Agent's Decision:\n",
            "A timely question!\n",
            "\n",
            "If a customer is having trouble with their order, it's always best to respond promptly and professionally to address their concern. Many customers will simply want to resolve the issue quickly and easily, so a swift and helpful response can go a long way in keeping them satisfied and loyal.\n",
            "\n",
            "Before escalating the issue, consider offering support and troubleshooting steps to help resolve the problem. This might include:\n",
            "\n",
            "1. Asking the customer to provide more information about the issue, such as order details or a description of the problem they're experiencing.\n",
            "2. Offering to resend or cancel the order, depending on the customer's needs.\n",
            "3. Providing troubleshooting steps or explanations to help the customer resolve the issue themselves.\n",
            "\n",
            "Some examples of responses you could send:\n",
            "\n",
            "* \"Sorry to hear you're having trouble with your order. Can you please provide more information about the issue, such as order number or a description of the problem you're experiencing? I'll do my best to help resolve this for you.\"\n",
            "* \"I apologize for the issue with your order. Would you like me to resend the order or cancel it for you? Please let me know how I can best assist you.\"\n",
            "\n",
            "If, after responding and providing support, the issue still cannot be resolved, you should consider escalating it to a supervisor, team leader, or specialized support team (e.g., for complex technical issues). This ensures that the customer's concern gets the attention it needs and that a resolution is reached.\n",
            "\n",
            "Remember, prompt and helpful responses can make all the difference in customer satisfaction and loyalty.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Memory - Chat History Recall"
      ],
      "metadata": {
        "id": "32bOI4QXS6oK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Previous user query for context\n",
        "previous_query = \"I'm having trouble with my order.\"\n",
        "print(f\"Previous user query: {previous_query}\")\n",
        "\n",
        "\n",
        "# Current query relying on previous context\n",
        "current_query = \"Is the trouble fixed?\"\n",
        "print(f\"Current query: {current_query}\")\n",
        "\n",
        "# Simulate memory recall for context-aware response\n",
        "memory_response = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Recall the user's previous query: '{previous_query}'. Now respond to: '{current_query}'.\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama3-8b-8192\",\n",
        ")\n",
        "print(\"Request for context-aware response sent to the LLM.\")\n",
        "\n",
        "# Print memory-based response\n",
        "print(\"Context-Aware Response:\")\n",
        "print(memory_response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeGBUTbpS18D",
        "outputId": "0ce6fb74-de7e-4940-e0ea-03fb59b63704"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Previous user query: I'm having trouble with my order.\n",
            "Current query: Is the trouble fixed?\n",
            "Request for context-aware response sent to the LLM.\n",
            "Context-Aware Response:\n",
            "I'm happy to help! I recall that you were having trouble with your order. I'm glad you asked if the trouble is fixed! Can you please provide more details about what happened with your order? Was it not delivered, or was there an issue with the product itself? The more information you can provide, the better I'll be able to assist you with resolving the issue.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Retrieval - Simulating retrieval from an external source"
      ],
      "metadata": {
        "id": "AFa77vzgTQQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "external_query = \"What is the latest news on language models?\"\n",
        "external_response = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": external_query,\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama3-8b-8192\",\n",
        ")\n",
        "\n",
        "print(\"External Retrieval Response:\", external_response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VXFuo9CMYOs",
        "outputId": "0564da25-7bb0-459d-e3e6-b894b779508a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "External Retrieval Response: Language models have been rapidly advancing in recent years, and there have been several significant developments and breakthroughs in the field. Here are some of the latest news and updates:\n",
            "\n",
            "1. **LLaMA: Meta's new large language model**: Meta AI has released LLaMA, a new large language model that has achieved state-of-the-art results on various Natural Language Processing (NLP) tasks, including language translation, question answering, and text generation.\n",
            "2. **BERT-based models surpass human performance**: Researchers have developed BERT-based models that have surpassed human performance on various NLP tasks, such as reading comprehension, sentiment analysis, and natural language inference.\n",
            "3. **AI-generated text is getting more realistic**: Researchers have made significant progress in generating realistic text using AI models, with some models able to generate text that is almost indistinguishable from human-written text.\n",
            "4. **Deep learning models for spoken language understanding**: Researchers have developed deep learning models that can understand spoken language, allowing for more accurate speech recognition and natural language processing applications.\n",
            "5. **Multimodal language models**: Researchers have been exploring multimodal language models that can process and understand both text and video data, enabling applications such as video captioning and language-based video search.\n",
            "6. **Explainability and transparency in language models**: Researchers have been working on making language models more transparent and explainable, which is essential for building trust in AI systems.\n",
            "7. **Fairness and bias in language models**: Researchers have been investigating the issue of fairness and bias in language models, which can have a significant impact on the accuracy and fairness of AI applications.\n",
            "8. **Language models for low-resource languages**: Researchers have been developing language models for low-resource languages, which can help bridge the language gap and enable AI applications in more languages.\n",
            "9. **Attention mechanisms for language models**: Researchers have been exploring attention mechanisms for language models, which can help improve the accuracy and efficiency of NLP applications.\n",
            "10. **Graph-based language models**: Researchers have been developing graph-based language models that can better capture complex relationships between words and entities, enabling applications such as text classification and sentiment analysis.\n",
            "\n",
            "Some notable recent publications and research articles in the field of language models include:\n",
            "\n",
            "* **\"LLaMA: Open and Efficient Foundation Language Models\"**: Meta AI (2022)\n",
            "* **\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\"**: Google Research (2018)\n",
            "* **\"Attention is All You Need\"**: Vaswani et al. (2017)\n",
            "* **\"Graph-based Neural Models for Natural Language Processing\"**: Yang et al. (2020)\n",
            "\n",
            "These advancements in language models have significant implications for various applications, including natural language processing, machine translation, chatbots, and assistive technology.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Indexing - Document Search"
      ],
      "metadata": {
        "id": "lpfU2xkFTiVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate indexing documents related to deep learning\n",
        "documents = {\n",
        "    \"doc1\": \"This document explains the basics of deep learning and neural networks.\",\n",
        "    \"doc2\": \"This document covers popular deep learning frameworks like TensorFlow and PyTorch.\",\n",
        "    \"doc3\": \"This document discusses the applications of deep learning in image recognition and natural language processing.\"\n",
        "}\n",
        "print(\"Documents indexed for search on deep learning.\")\n",
        "\n",
        "# User search query\n",
        "search_query = \"deep learning\"\n",
        "print(f\"User search query: {search_query}\")\n",
        "\n",
        "# Search through indexed documents\n",
        "search_response = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Search for documents containing: '{search_query}'.\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama3-8b-8192\",\n",
        ")\n",
        "print(\"Request for document search sent to the LLM.\")\n",
        "\n",
        "# Print search results\n",
        "print(\"Search Results:\")\n",
        "print(search_response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cM7FvQUWTVXc",
        "outputId": "bbcf5063-cf4d-424d-fbcd-db70f29dff3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documents indexed for search on deep learning.\n",
            "User search query: deep learning\n",
            "Request for document search sent to the LLM.\n",
            "Search Results:\n",
            "I've searched for documents containing the phrase \"deep learning\". Here are some results:\n",
            "\n",
            "1. **Deep Learning** by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (2016) - This is a comprehensive textbook on deep learning, widely considered a seminal work in the field.\n",
            "\n",
            "Source: Book excerpt on GitHub\n",
            "Text: \"Deep learning is a subfield of machine learning that employs neural networks with multiple layers to learn hierarchical representations of data\".\n",
            "\n",
            "2. **Deep Learning for Computer Vision with Python** by Adrian Rosebrock (2017) - This is a book that covers the application of deep learning to computer vision tasks using Python.\n",
            "\n",
            "Source: Book excerpt on GitHub\n",
            "Text: \"In this chapter, we'll explore the basics of deep learning and how it can be used to create advanced computer vision systems\".\n",
            "\n",
            "3. Research paper: **Deep Learning to Classify Invasive Diseases** by Liu et al. (2020) - This paper describes a deep learning model for classification of invasive diseases, such as cancer.\n",
            "\n",
            "Source: arXiv preprint\n",
            "Text: \"In this study, we propose a deep learning framework for identifying invasive diseases from high-dimensional genomic data\".\n",
            "\n",
            "4. **Deep Learning for Natural Language Processing** by Yoav Goldberg (2016) - This is a survey paper on the application of deep learning to natural language processing tasks, such as language modeling and machine translation.\n",
            "\n",
            "Source: ACL anthology\n",
            "Text: \"Deep learning has revolutionized the field of natural language processing, enabling significant improvements in many tasks\".\n",
            "\n",
            "5. **Deep Learning Specialization** by Andrew Ng on Coursera (2019) - This is a online course series on deep learning, covering topics such as neural networks, convolutional networks, and recurrent networks.\n",
            "\n",
            "Source: Course syllabus on Coursera\n",
            "Text: \"In this specialization, you'll learn the fundamentals of deep learning, including how to implement and apply deep learning models to computer vision, natural language processing, and other applications\".\n",
            "\n",
            "These documents represent just a few of the many resources available on deep learning. If you'd like to explore more, please let me know!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Agent Framework in LangChain\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4ZcIjeMdReoQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Personalized News Aggregator**\n",
        "\n",
        "**News API Key** - https://newsapi.org/"
      ],
      "metadata": {
        "id": "LY3fEQAXByWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq feedparser\n",
        "!pip install langchain groq newsapi-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xKdoMX7-wEmx",
        "outputId": "80239fd0-ba9d-498f-f29a-f92c21fd64f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.11.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from groq)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Collecting sgmllib3k (from feedparser)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.23.4)\n",
            "Downloading groq-0.11.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=bc286bd28078960ba4c1c7b09c4155ba54d5e6fe24ecc85bd7ef9e0a2e87e461\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, h11, feedparser, httpcore, httpx, groq\n",
            "Successfully installed feedparser-6.0.11 groq-0.11.0 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 sgmllib3k-1.0.0\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.4-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: groq in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
            "Collecting newsapi-python\n",
            "  Downloading newsapi_python-0.2.7-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.4.0,>=0.3.12 (from langchain)\n",
            "  Downloading langchain_core-0.3.12-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.136-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq) (0.27.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.15.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.12->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (24.1)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
            "Downloading langchain-0.3.4-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading newsapi_python-0.2.7-py2.py3-none-any.whl (7.9 kB)\n",
            "Downloading langchain_core-0.3.12-py3-none-any.whl (407 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.7/407.7 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.136-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.7/296.7 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading orjson-3.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: orjson, jsonpointer, requests-toolbelt, newsapi-python, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.4 langchain-core-0.3.12 langchain-text-splitters-0.3.0 langsmith-0.1.136 newsapi-python-0.2.7 orjson-3.10.9 requests-toolbelt-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from groq import Groq\n",
        "from newsapi import NewsApiClient\n",
        "os.environ[\"GROQ_API_KEY\"] = \"Place Your API Key Here\"\n",
        "os.environ[\"NEWS_API_KEY\"] = \"Place Your News API Key Here\""
      ],
      "metadata": {
        "id": "WylJYFYcxtPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### News Retriever Agent: Responsible for fetching news articles based on user queries\n"
      ],
      "metadata": {
        "id": "g4nwxVMPTFBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of How Agents Work in the Personalized News Aggregator:**\n",
        "\n",
        "1. **NewsRetrieverAgent:** This agent fetches news articles based on the user’s input. It takes the search query, retrieves relevant articles from the News API, and filters them based on the keywords.\n",
        "\n",
        "2. **UserSelectionAgent:** This agent presents the list of retrieved articles to the user and allows them to select one for reading. It manages user input and ensures valid selection.\n",
        "\n",
        "3. **SummarizationAgent:** This agent takes the content of the selected article and summarizes it using llama3. It ensures the content is suitable for summarization and processes the request to generate a concise summary.\n",
        "\n",
        "4. **Presentation Agent:** Formats and delivers news."
      ],
      "metadata": {
        "id": "5AncuH-uUyq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NewsRetrieverAgent:\n",
        "    def __init__(self, api_key):\n",
        "        # Initialize the News API client with the provided API key\n",
        "        self.newsapi = NewsApiClient(api_key=api_key)\n",
        "\n",
        "    def get_news(self, query_tokens, language='en', page_size=5):\n",
        "        try:\n",
        "            # Fetch articles based on the provided tokens\n",
        "            all_articles = self.newsapi.get_everything(\n",
        "                q=' '.join(query_tokens),  # Create a query from tokens\n",
        "                language=language,\n",
        "                page_size=100,  # Maximum number of articles to retrieve\n",
        "                sort_by=\"publishedAt\"\n",
        "            )\n",
        "\n",
        "            # Check if the API response is successful\n",
        "            if all_articles['status'] != 'ok':\n",
        "                print(f\"Error fetching articles: {all_articles['message']}\")\n",
        "                return []\n",
        "\n",
        "            # Filter articles that contain all tokens in the title or description\n",
        "            relevant_articles = [\n",
        "                article for article in all_articles['articles']\n",
        "                if all(token.lower() in article['title'].lower() or token.lower() in article['description'].lower() for token in query_tokens)\n",
        "            ]\n",
        "            return relevant_articles[:page_size]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while retrieving news: {e}\")\n",
        "            return []"
      ],
      "metadata": {
        "id": "URMHesHN2xPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### User Selection Agent: Allows users to select an article from the retrieved articles"
      ],
      "metadata": {
        "id": "kvmOQnAzTxD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UserSelectionAgent:\n",
        "    @staticmethod\n",
        "    def select_article(articles):\n",
        "        if not articles:\n",
        "            print(\"No articles available to select.\")\n",
        "            return None\n",
        "\n",
        "        print(\"\\nHere are the latest articles:\")\n",
        "        for i, article in enumerate(articles):\n",
        "            print(f\"{i + 1}. {article['title']}\")\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                # Prompt user to select an article by entering its number\n",
        "                selection = int(input(\"\\nSelect the number of the article you want to read: \")) - 1\n",
        "                if 0 <= selection < len(articles):\n",
        "                    return articles[selection]  # Return the selected article\n",
        "                else:\n",
        "                    print(\"Invalid selection, please try again.\")\n",
        "            except ValueError:\n",
        "                print(\"Please enter a valid number.\")"
      ],
      "metadata": {
        "id": "3al7arFhTvKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summarization Agent: Summarizes the content of the selected article"
      ],
      "metadata": {
        "id": "Y6xW9TIaT_Iw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SummarizationAgent:\n",
        "    def __init__(self):\n",
        "        self.client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
        "\n",
        "    def summarize(self, text):\n",
        "        # Check if the content is too short to summarize\n",
        "        if len(text) < 100:\n",
        "            return \"The content is too short for summarization.\"\n",
        "        # Create a chat completion request for summarization\n",
        "        chat_completion = self.client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"Summarize the following news article:\\n\\n{text}\\n\\nSummary:\",\n",
        "                }\n",
        "            ],\n",
        "            model=\"llama3-8b-8192\",\n",
        "        )\n",
        "        return chat_completion.choices[0].message.content.strip()"
      ],
      "metadata": {
        "id": "HW3t6td3T6fL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Pipeline: Coordinates the entire process"
      ],
      "metadata": {
        "id": "dZf97h31UNn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Step 1: Instantiate the NewsRetrieverAgent\n",
        "    news_retriever = NewsRetrieverAgent(api_key=os.environ[\"NEWS_API_KEY\"])\n",
        "\n",
        "    # Step 2: Get user input and tokenize\n",
        "    query = input(\"Enter a topic or person (e.g., 'Elon Musk') to search for news: \")\n",
        "    query_tokens = query.split()\n",
        "\n",
        "    # Step 3: Retrieve news articles related to the tokens\n",
        "    articles = news_retriever.get_news(query_tokens=query_tokens)\n",
        "    if not articles:\n",
        "        print(f\"No news found for {query}. Please try again.\")\n",
        "        return\n",
        "\n",
        "    # Step 4: Instantiate the UserSelectionAgent and let the user pick an article\n",
        "    user_selection_agent = UserSelectionAgent()\n",
        "    selected_article = user_selection_agent.select_article(articles)\n",
        "    if selected_article is None:\n",
        "        print(\"No article selected. Exiting.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nSelected Article Details:\")\n",
        "    print(f\"Title: {selected_article['title']}\")\n",
        "    print(f\"Source: {selected_article['source']['name']}\")\n",
        "    print(f\"Published At: {selected_article['publishedAt']}\")\n",
        "    print(f\"Description: {selected_article['description']}\")\n",
        "    print(f\"Content: {selected_article['content']}\")\n",
        "\n",
        "    # Step 5: Instantiate the SummarizationAgent and summarize the selected article\n",
        "    summarizer = SummarizationAgent()\n",
        "\n",
        "    # Summarize the article content\n",
        "    summary = summarizer.summarize(selected_article['content'])\n",
        "    print(\"\\nSummary:\",summary)"
      ],
      "metadata": {
        "id": "pIOzWTm8ULsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZVU3RXG35pl",
        "outputId": "cbbb87a4-baf5-4b90-e7ff-8ab8e69e533d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a topic or person (e.g., 'Elon Musk') to search for news: elon musk new car\n",
            "\n",
            "Here are the latest articles:\n",
            "1. Elon Musk’s New Robot Venture Is Essentially Cartman’s AWESOM-O Prank\n",
            "2. Jim Cramer Advises Against Buying Tesla Stock, So I Guess It's Going Up Forever\n",
            "3. Elon Musk’s Tesla Moves $760,000,000 in Bitcoin to New Wallets for First Time in Two Years: Arkham Intelligence\n",
            "4. Is Elon Musk Dumping Bitcoin? Tesla Moves Entire $765 Million BTC Stash To Unknown Wallets\n",
            "5. Here's What We Think of Tesla's New Cybercab\n",
            "\n",
            "Select the number of the article you want to read: 5\n",
            "\n",
            "Selected Article Details:\n",
            "Title: Here's What We Think of Tesla's New Cybercab\n",
            "Source: Biztoc.com\n",
            "Published At: 2024-10-14T15:11:25Z\n",
            "Description: In this bonus episode of the Elon, Inc. podcast, we discuss Tesla's unveiling of the long-awaited robotaxi, and why it may be even more challenging to put a safe self-driving car on the road than it is to send rockets to space. #musk #tesla #cybercab #robotax…\n",
            "Content: In this bonus episode of the Elon, Inc. podcast, we discuss Tesla's unveiling of the long-awaited robotaxi, and why it may be even more challenging to put a safe self-driving car on the road than it … [+134 chars]\n",
            "\n",
            "Summary: The bonus episode of the Elon, Inc. podcast discusses Tesla's unveiling of its robotaxi, a self-driving car, and the challenges of making it safe for the road.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUEf0Ki9xxsv",
        "outputId": "a474b97f-25f1-490f-8ba8-13405302a117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a topic or person (e.g., 'Elon Musk') to search for news: Baba Siddique\n",
            "\n",
            "Here are the latest articles:\n",
            "1. Baba Siddique murder case: Another suspect held for funding shooters\n",
            "2. Cops Make 4th Arrest In Baba Siddique Murder Case, Catch Accused From UP\n",
            "3. The Lawrence of Sabarmati jail: Bishnoi's name echoes from Mumbai to Canada\n",
            "4. Baba Siddique: What we know so far about public murder of an Indian politician - BBC.com\n",
            "5. 'Zulmi' Zeeshan: What Cop Records Show On Baba Siddique Shooters' Handler\n",
            "\n",
            "Select the number of the article you want to read: 1\n",
            "\n",
            "Selected Article Details:\n",
            "Title: Baba Siddique murder case: Another suspect held for funding shooters\n",
            "Source: Business Standard\n",
            "Published At: 2024-10-15T10:09:18Z\n",
            "Description: The Mumbai police have arrested one more person in connection with the killing of NCP leader and former Maharashtra minister Baba Siddique, officials said on Tuesday.\n",
            "The accused, identified as Harishkumar Balakram (23), hailing from Bahraich in Uttar Pradesh…\n",
            "Content: The Mumbai police have arrested one more person in connection with the killing of NCP leader and former Maharashtra minister Baba Siddique, officials said on Tuesday.\n",
            "The accused, identified as Hari… [+2800 chars]\n",
            "\n",
            "Summary:\n",
            "The Mumbai police have arrested another person in connection with the murder of NCP leader and former Maharashtra minister Baba Siddique.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFwYfalJ3Tw8",
        "outputId": "6e713b4c-1061-4963-a745-74c600e99c88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a topic or person (e.g., 'Elon Musk') to search for news: tata's heir \n",
            "\n",
            "Here are the latest articles:\n",
            "1. Tata Trusts meeting today: Decision about successor on agenda, Noel Tata most likely heir to Ratan Tata, say reports\n",
            "\n",
            "Select the number of the article you want to read: 1\n",
            "\n",
            "Selected Article Details:\n",
            "Title: Tata Trusts meeting today: Decision about successor on agenda, Noel Tata most likely heir to Ratan Tata, say reports\n",
            "Source: Livemint\n",
            "Published At: 2024-10-11T07:28:36Z\n",
            "Description: The Tata Trusts board is meeting today to select Ratan Tata's successor — a pivotal role overseeing a $165 billion empire, with potential contenders including Noel Tata, Mehli Mistry, and Darius Khambata.\n",
            "Content: Ahead of the Tata Trust meeting today on October 11, the late Ratan Tata's half-brother Noel Tata (67) has emerged as the front-runner to be the successor.\n",
            "Bloomberg reported, citing sources in the … [+2892 chars]\n",
            "\n",
            "Summary:\n",
            "Ahead of a Tata Trust meeting on October 11, Noel Tata, the 67-year-old half-brother of late Ratan Tata, is reportedly the top contender to succeed him.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI Automations by LangChain\n",
        "\n"
      ],
      "metadata": {
        "id": "EMFhqqUbVigH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interview Preparation Assistant\n",
        "\n",
        "**Job Info -** [Job Details](https://www.naukri.com/job-listings-principal-data-scientist-senior-data-scientist-data-scientist-nlp-benovymed-healthcare-noida-gurugram-delhi-ncr-0-to-5-years-151024914631?src=drecomm_mightlike&sid=17291039228452039&xp=1&px=1)\n",
        "\n",
        "User Inputs Job Description -> Analyze Job Description -> Generate Interview Questions -> Extract Questions -> Conduct Interview Process (Ask Questions) -> Provide Feedback -> Display Overall Score and Improvement Areas\n",
        "\n",
        "\n",
        "**How AI Automation Works:**\n",
        "\n",
        "**Chaining Tasks:** In this application, tasks are chained together where the output of one step serves as the input for the next.\n",
        "\n",
        "**Human-Like Decision-Making:** The program simulates a human-like interview process by asking questions, receiving answers, and providing tailored feedback based on user responses.\n",
        "\n",
        "**External Data Interaction:** The integration of Groq's API for language model processing allows for advanced text generation and analysis, enhancing the capabilities of the interview preparation assistant."
      ],
      "metadata": {
        "id": "NELxCrKPBL-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UcJ5kgPUmUGn",
        "outputId": "1da55fa2-e8eb-4829-ee7c-aee4430903ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.3.4-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groq\n",
            "  Downloading groq-0.11.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.4.0,>=0.3.12 (from langchain)\n",
            "  Downloading langchain_core-0.3.12-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.136-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from groq)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.15.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.12->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (24.1)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
            "Downloading langchain-0.3.4-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groq-0.11.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.12-py3-none-any.whl (407 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.7/407.7 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.136-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.7/296.7 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading orjson-3.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: orjson, jsonpointer, h11, requests-toolbelt, jsonpatch, httpcore, httpx, langsmith, groq, langchain-core, langchain-text-splitters, langchain\n",
            "Successfully installed groq-0.11.0 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.4 langchain-core-0.3.12 langchain-text-splitters-0.3.0 langsmith-0.1.136 orjson-3.10.9 requests-toolbelt-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up GROQ_API_KEY and Initialize the Groq client\n"
      ],
      "metadata": {
        "id": "VHPBsVn5uUI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from langchain.prompts import PromptTemplate\n",
        "from groq import Groq\n",
        "\n",
        "# Set the GROQ_API_KEY directly in the environment\n",
        "os.environ[\"GROQ_API_KEY\"] = \"Place Your API Key Here\"\n",
        "\n",
        "# Initialize the Groq client for LLM processing\n",
        "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))"
      ],
      "metadata": {
        "id": "doYDsyiU4A17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Automated Interview Preparation Assistant Prompts and Utilities"
      ],
      "metadata": {
        "id": "pc9BhwmQuZxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Define the prompt for Job Description Analyzer Agent\n",
        "# This agent extracts key responsibilities, skills, and qualifications from the provided job description.\n",
        "\n",
        "job_description_analyzer_template = \"\"\"\n",
        "Given the following job description, extract the key responsibilities, skills, and qualifications required for the job.\n",
        "Job Description: {job_description}\n",
        "\"\"\"\n",
        "job_description_analyzer_prompt = PromptTemplate(\n",
        "    template=job_description_analyzer_template, input_variables=[\"job_description\"]\n",
        ")\n",
        "\n",
        "# Step 2: Define the Interview Question Generator Prompt\n",
        "# This prompt generates interview questions based on the analyzed job description, role, and difficulty level.\n",
        "\n",
        "interview_question_generator_template = \"\"\"\n",
        "Based on the extracted responsibilities, skills, and qualifications for a {role} role at a {difficulty} level, generate a list of five interview questions.\n",
        "\"\"\"\n",
        "interview_question_prompt = PromptTemplate(\n",
        "    template=interview_question_generator_template, input_variables=[\"role\", \"difficulty\"]\n",
        ")\n",
        "\n",
        "# Step 3: Feedback Agent Prompt for scoring and feedback\n",
        "# This agent provides feedback on a user's mock interview answer, focusing on strengths, weaknesses, and score.\n",
        "\n",
        "feedback_agent_template = \"\"\"\n",
        "Here is a mock interview answer: {answer}\n",
        "Provide feedback on this answer, focusing on strengths, weaknesses, and improvements. Assign a score between 0 and 10.\n",
        "\"\"\"\n",
        "feedback_agent_prompt = PromptTemplate(\n",
        "    template=feedback_agent_template, input_variables=[\"answer\"]\n",
        ")\n",
        "\n",
        "# Step 4: Function to extract individual questions from the generated text\n",
        "# This uses regex to extract only the actual questions from the generated output, excluding explanations.\n",
        "\n",
        "def extract_questions(generated_text):\n",
        "    # Use regex to find all questions in the generated text\n",
        "    questions = re.findall(r'\\*\\*Question \\d:\\*\\* (.*?)(?=\\n\\n|\\Z)', generated_text, re.DOTALL)\n",
        "    return [question.strip() for question in questions]"
      ],
      "metadata": {
        "id": "Hn2fowuiu78F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Chained Workflow for Interview Preparation"
      ],
      "metadata": {
        "id": "nmVHGrPx99JW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chained Workflow for Interview Preparation:**\n",
        "\n",
        "1. **Analyze Job Description:** The job description is sent to the API, and the response is analyzed to extract key information.\n",
        "\n",
        "2. **Generate Interview Questions:** Based on the extracted information, interview questions are generated.\n",
        "\n",
        "3. **Extract Questions:** The generated questions are extracted and displayed to the user."
      ],
      "metadata": {
        "id": "g6W3ftWH_q3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interview_preparation_workflow(job_description, role, difficulty):\n",
        "    # 1. Analyze the job description\n",
        "    prompt = job_description_analyzer_prompt.format(job_description=job_description)\n",
        "    response = client.chat.completions.create(\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        model=\"llama3-8b-8192\"\n",
        "    )\n",
        "    analyzed_info = response.choices[0].message.content\n",
        "    print(\"Analyzed Job Description: \\n\", analyzed_info)\n",
        "\n",
        "    # 2. Generate interview questions\n",
        "    question_prompt = interview_question_prompt.format(role=role, difficulty=difficulty)\n",
        "    response = client.chat.completions.create(\n",
        "        messages=[{\"role\": \"user\", \"content\": question_prompt}],\n",
        "        model=\"llama3-8b-8192\"\n",
        "    )\n",
        "    generated_questions = response.choices[0].message.content\n",
        "\n",
        "    # 3. Extract individual questions from the generated response\n",
        "    questions = extract_questions(generated_questions)\n",
        "    if questions:\n",
        "        print(\"Generated Interview Questions:\")\n",
        "        for i, question in enumerate(questions):\n",
        "            print(f\"Question {i + 1}: {question}\")\n",
        "    return questions"
      ],
      "metadata": {
        "id": "sWlo_HDQ3SRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Step 6: User Interaction for the Interview Preparation\n",
        "\n",
        "Manages user interaction for the interview process, including asking questions, getting answers, providing feedback, and calculating scores."
      ],
      "metadata": {
        "id": "XXSEFltyAYqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conduct_interview(questions):\n",
        "    overall_score = 0\n",
        "    total_questions = len(questions)\n",
        "    improvement_areas = []\n",
        "    question_index = 0\n",
        "    # Display total questions available before starting the interview\n",
        "    print(f\"Total Questions Available: {total_questions}\")\n",
        "    # Ensure there are questions to ask\n",
        "    if total_questions > 0:\n",
        "        while question_index < total_questions:\n",
        "            question = questions[question_index].strip()\n",
        "            print(f\"\\nQuestion {question_index + 1}: {question}\")\n",
        "            knows_answer = input(\"Do you know the answer to this question? (yes/no): \").strip().lower()\n",
        "            if knows_answer == \"yes\":\n",
        "                # Ask the user to enter their answer\n",
        "                answer = input(\"Please enter your answer: \")\n",
        "\n",
        "                # Provide feedback and score the answer\n",
        "                feedback = assistant.provide_feedback(answer)\n",
        "                print(f\"Feedback on your answer: \\n{feedback}\")\n",
        "\n",
        "                # Extract score from the feedback\n",
        "                score_line = [line for line in feedback.split('\\n') if 'Score:' in line]\n",
        "                print(\"Score Line Detected:\", score_line)  # Debugging output\n",
        "                if score_line:\n",
        "                    try:\n",
        "                        score = float(score_line[0].split(\":\")[1].strip())  # Adjusted to only convert score\n",
        "                    except ValueError:\n",
        "                        score = 0\n",
        "                else:\n",
        "                    score = 0\n",
        "                overall_score += score\n",
        "                if \"improvement\" in feedback.lower():\n",
        "                    improvement_areas.append(feedback)\n",
        "            else:\n",
        "                print(\"Skipping this question...\")\n",
        "\n",
        "            # Ask if they want another question\n",
        "            another_question = input(\"Do you want another question? (yes/no): \").strip().lower()\n",
        "            if another_question != \"yes\":\n",
        "                break\n",
        "            question_index += 1\n",
        "    average_score = overall_score / total_questions if total_questions > 0 else 0\n",
        "    print(f\"\\nOverall Score: {average_score:.2f}/10\")\n",
        "    if improvement_areas:\n",
        "        print(\"\\nAreas to Improve:\")\n",
        "        for improvement in improvement_areas:\n",
        "            print(f\"- {improvement}\")\n",
        "    else:\n",
        "        print(\"No major areas of improvement identified. Keep practicing!\")"
      ],
      "metadata": {
        "id": "5Ynl2ulHAROm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 7: Get user input for job description, role, and difficulty"
      ],
      "metadata": {
        "id": "50aub3R1-RMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "job_description = input(\"Please enter the job description: \")\n",
        "role = input(\"Please enter the job title/role based on the job description: \")\n",
        "difficulty = input(\"Please select the difficulty level (easy, medium, hard): \").strip().lower()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6ha3IZD-Ti7",
        "outputId": "72564420-0223-4006-d33e-55f43fb57c1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter the job description: We are looking for passionate, Strong experience with an entrepreneur mindset to join us as Hardcore Full Stack Data Scientist end-to-end single-handed multi-hat role who is already working in applied AI in ML, deep Learning, ANN, and CNN platform specifically working as single-handed doers or in a small Data Science Team with full end to end ownership of Data Science work commitment delivery as Data Scientist working in any reputed AI Data Science is driven Tech startup preferably in Healthcare.\n",
            "Please enter the job title/role based on the job description: Data Scientist\n",
            "Please select the difficulty level (easy, medium, hard): easy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions = interview_preparation_workflow(job_description, role, difficulty)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8iDthkO6SAn",
        "outputId": "3b8c96f1-b3c1-48a0-ab76-1c64e96a573a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzed Job Description: \n",
            " Here are the key responsibilities, skills, and qualifications required for the job:\n",
            "\n",
            "**Key Responsibilities:**\n",
            "\n",
            "* Working as a single-handed Data Scientist, with full end-to-end ownership of Data Science work and commitment to delivery\n",
            "* Developing and implementing end-to-end data science solutions using applied AI, ML, deep learning, ANN, and CNN platforms\n",
            "* Driving data science projects in a small team or as a single-handed doer\n",
            "* Providing data-driven insights and recommendations to improve business outcomes\n",
            "\n",
            "**Skills:**\n",
            "\n",
            "* Strong experience with deep learning and AI platforms\n",
            "* Expertise in ML, ANN, and CNN algorithms\n",
            "* Proficiency in programming languages such as Python\n",
            "* Strong data science skills, including data preprocessing, feature engineering, and model development\n",
            "* Experience working with large datasets and building data-driven products\n",
            "* Excellent problem-solving and analytical skills\n",
            "* Ability to work independently and deliver end-to-end ownership of projects\n",
            "\n",
            "**Qualifications:**\n",
            "\n",
            "* Bachelor's or Master's degree in Computer Science, Statistics, Mathematics, or a related field\n",
            "* Prior experience working as a Data Scientist in a reputed AI Data Science is driven Tech startup, preferably in the Healthcare sector\n",
            "* Strong entrepreneurial mindset and willingness to take ownership of projects and delivery\n",
            "* Ability to work in a fast-paced environment and adapt to changing priorities\n",
            "* Strong communication and collaboration skills, with the ability to work with cross-functional teams.\n",
            "Generated Interview Questions:\n",
            "Question 1: Can you explain the concept of data preprocessing and provide an example of how you would handle missing values in a dataset?\n",
            "Question 2: Suppose you have two datasets with different scales. How would you combine them into a single dataset for analysis?\n",
            "Question 3: What is your approach to feature selection, and can you give an example of when you would use it in a project?\n",
            "Question 4: Can you describe a situation where you would use regression analysis, and how would you interpret the results?\n",
            "Question 5: How do you ensure that the insights you derive from data are reliable and actionable? Can you give an example of a project where this was crucial?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conduct_interview(questions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZ7KW-xB6UEf",
        "outputId": "6998f75e-c3fb-4665-976a-240ff1c4030e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Questions Available: 5\n",
            "\n",
            "Question 1: Can you explain the concept of data preprocessing and provide an example of how you would handle missing values in a dataset?\n",
            "Do you know the answer to this question? (yes/no): yes\n",
            "Please enter your answer: Data preprocessing involves cleaning and transforming raw data into a usable format, ensuring accuracy and consistency. For example, I have used normalization to scale numerical features to a common range, enhancing model performance.\n",
            "Feedback on your answer: \n",
            "Here's the feedback on the mock interview answer:\n",
            "\n",
            "**Strengths:** (6/10)\n",
            "\n",
            "* The answer starts by defining what data preprocessing is, which is important for non-technical interviewers.\n",
            "* The example provided is specific and related to data preprocessing.\n",
            "* The candidate mentions the benefits of normalization, such as enhancing model performance.\n",
            "\n",
            "**Weaknesses:** (4/10)\n",
            "\n",
            "* The answer is quite generic and doesn't provide any unique insights or personal experiences.\n",
            "* The candidate doesn't explain why normalization is important or how it affected the model performance. It would be more useful to discuss the specific problem that normalization solved or the improvements it brought.\n",
            "* The answer is quite short and doesn't elaborate on the candidate's role in the process.\n",
            "\n",
            "**Improvements:**\n",
            "\n",
            "* Provide more context about the problem you were trying to solve and how data preprocessing helped.\n",
            "* Explain the specific steps you took during the data preprocessing process and any challenges you faced.\n",
            "* Quantify the benefits of normalization, such as \"normalizing the data reduced the mean squared error from 0.5 to 0.2\".\n",
            "* Highlight your contribution to the project, such as \"I was responsible for data preprocessing, and my efforts resulted in a 20% improvement in model accuracy\".\n",
            "* Make the answer more concrete and specific, avoiding generic phrases like \"ensuring accuracy and consistency\".\n",
            "\n",
            "**Score:** 5/10\n",
            "\n",
            "This answer is a good starting point, but it could benefit from more details and specificity. By providing more context, explaining the benefits of normalization, and highlighting their contributions, the candidate can make their answer more compelling and memorable.\n",
            "Score Line Detected: ['**Score:** 5/10']\n",
            "Do you want another question? (yes/no): yes\n",
            "\n",
            "Question 2: Suppose you have two datasets with different scales. How would you combine them into a single dataset for analysis?\n",
            "Do you know the answer to this question? (yes/no): no\n",
            "Skipping this question...\n",
            "Do you want another question? (yes/no): yes\n",
            "\n",
            "Question 3: What is your approach to feature selection, and can you give an example of when you would use it in a project?\n",
            "Do you know the answer to this question? (yes/no): yes\n",
            "Please enter your answer: hbiub hjdbiu sijn\n",
            "Feedback on your answer: \n",
            "What a... unique answer!\n",
            "\n",
            "**Strengths:** None, unfortunately. This answer doesn't contain any relevant information, keywords, or insightful thoughts. It's not clear what the candidate is trying to convey or what their qualifications are.\n",
            "\n",
            "**Weaknesses:** The answer is completely unintelligible, making it difficult to assess the candidate's communication skills, thought process, or expertise.\n",
            "\n",
            "**Improvements:** To make this answer more effective, the candidate could:\n",
            "\n",
            "1. Provide a clear and concise summary of their qualifications or experience related to the job.\n",
            "2. Focus on specific achievements, accomplishments, or skills that demonstrate their value as a candidate.\n",
            "3. Use proper grammar, spelling, and punctuation to ensure the answer is easy to understand.\n",
            "\n",
            "**Score:** 0/10\n",
            "\n",
            "This answer fails to demonstrate any of the essential skills or qualities required for a mock interview. While it's not a accurate representation of the candidate's abilities, I hope this feedback helps them improve their response for future interviews!\n",
            "Score Line Detected: ['**Score:** 0/10']\n",
            "Do you want another question? (yes/no): yes\n",
            "\n",
            "Question 4: Can you describe a situation where you would use regression analysis, and how would you interpret the results?\n",
            "Do you know the answer to this question? (yes/no): yes\n",
            "Please enter your answer: Regression analysis can be used to predict housing prices based on various factors like square footage and location. The results can be interpreted by examining the coefficients to understand the relationship between predictors and the target variable, indicating how changes in predictors affect the predicted outcome.\n",
            "Feedback on your answer: \n",
            "**Score: 7.5**\n",
            "\n",
            "**Strengths:**\n",
            "\n",
            "1. The answer clearly mentions the purpose of regression analysis, which is to predict housing prices based on various factors.\n",
            "2. It provides a specific example of how regression analysis can be applied in practice.\n",
            "3. The answer mentions the importance of interpreting the coefficients to understand the relationships between predictors and the target variable.\n",
            "\n",
            "**Weaknesses:**\n",
            "\n",
            "1. The answer is quite straightforward and lacks depth. It does not provide any additional context or insights into why regression analysis is particularly useful for this problem.\n",
            "2. The language used is quite general and lacks technical precision. For example, the phrase \"relationship between predictors and the target variable\" could be more specific, such as \"linear relationship between square footage and housing price\".\n",
            "3. The answer does not provide an example of what kind of results the researcher might see, such as a statistical significance value or a confidence interval.\n",
            "\n",
            "**Improvements:**\n",
            "\n",
            "1. Provide more context about why regression analysis is suitable for this problem. For example, you could discuss how it allows for the control of other factors that might affect housing prices.\n",
            "2. Use more technical language to describe the regression analysis. For example, you could discuss the types of coefficients (e.g. slope, intercept) and how they relate to the predicted outcome.\n",
            "3. Provide an example of what kind of results the researcher might see, such as a statistical significance value or a confidence interval. This would help to give a clearer idea of what the results mean.\n",
            "\n",
            "Here is an example of how the answer could be improved:\n",
            "\n",
            "\"Regression analysis can be a powerful tool for predicting housing prices based on various factors, such as square footage and location. By using regression analysis, we can control for other factors that might affect housing prices, such as proximity to public transportation or the quality of local schools. The results of the regression analysis can be interpreted by examining the coefficients, which represent the change in the predicted housing price for a one-unit change in the predictor variable, while holding all other factors constant. For example, if the coefficient for square footage is 0.05, this means that a 100 square foot increase in square footage would result in a 5% increase in the predicted housing price, assuming all other factors remain constant. The statistical significance of the regression analysis and the confidence intervals around the predicted values can also provide valuable insights into the reliability of the model.\"\n",
            "Score Line Detected: ['**Score: 7.5**']\n",
            "Do you want another question? (yes/no): no\n",
            "\n",
            "Overall Score: 0.00/10\n",
            "\n",
            "Areas to Improve:\n",
            "- Here's the feedback on the mock interview answer:\n",
            "\n",
            "**Strengths:** (6/10)\n",
            "\n",
            "* The answer starts by defining what data preprocessing is, which is important for non-technical interviewers.\n",
            "* The example provided is specific and related to data preprocessing.\n",
            "* The candidate mentions the benefits of normalization, such as enhancing model performance.\n",
            "\n",
            "**Weaknesses:** (4/10)\n",
            "\n",
            "* The answer is quite generic and doesn't provide any unique insights or personal experiences.\n",
            "* The candidate doesn't explain why normalization is important or how it affected the model performance. It would be more useful to discuss the specific problem that normalization solved or the improvements it brought.\n",
            "* The answer is quite short and doesn't elaborate on the candidate's role in the process.\n",
            "\n",
            "**Improvements:**\n",
            "\n",
            "* Provide more context about the problem you were trying to solve and how data preprocessing helped.\n",
            "* Explain the specific steps you took during the data preprocessing process and any challenges you faced.\n",
            "* Quantify the benefits of normalization, such as \"normalizing the data reduced the mean squared error from 0.5 to 0.2\".\n",
            "* Highlight your contribution to the project, such as \"I was responsible for data preprocessing, and my efforts resulted in a 20% improvement in model accuracy\".\n",
            "* Make the answer more concrete and specific, avoiding generic phrases like \"ensuring accuracy and consistency\".\n",
            "\n",
            "**Score:** 5/10\n",
            "\n",
            "This answer is a good starting point, but it could benefit from more details and specificity. By providing more context, explaining the benefits of normalization, and highlighting their contributions, the candidate can make their answer more compelling and memorable.\n",
            "- What a... unique answer!\n",
            "\n",
            "**Strengths:** None, unfortunately. This answer doesn't contain any relevant information, keywords, or insightful thoughts. It's not clear what the candidate is trying to convey or what their qualifications are.\n",
            "\n",
            "**Weaknesses:** The answer is completely unintelligible, making it difficult to assess the candidate's communication skills, thought process, or expertise.\n",
            "\n",
            "**Improvements:** To make this answer more effective, the candidate could:\n",
            "\n",
            "1. Provide a clear and concise summary of their qualifications or experience related to the job.\n",
            "2. Focus on specific achievements, accomplishments, or skills that demonstrate their value as a candidate.\n",
            "3. Use proper grammar, spelling, and punctuation to ensure the answer is easy to understand.\n",
            "\n",
            "**Score:** 0/10\n",
            "\n",
            "This answer fails to demonstrate any of the essential skills or qualities required for a mock interview. While it's not a accurate representation of the candidate's abilities, I hope this feedback helps them improve their response for future interviews!\n",
            "- **Score: 7.5**\n",
            "\n",
            "**Strengths:**\n",
            "\n",
            "1. The answer clearly mentions the purpose of regression analysis, which is to predict housing prices based on various factors.\n",
            "2. It provides a specific example of how regression analysis can be applied in practice.\n",
            "3. The answer mentions the importance of interpreting the coefficients to understand the relationships between predictors and the target variable.\n",
            "\n",
            "**Weaknesses:**\n",
            "\n",
            "1. The answer is quite straightforward and lacks depth. It does not provide any additional context or insights into why regression analysis is particularly useful for this problem.\n",
            "2. The language used is quite general and lacks technical precision. For example, the phrase \"relationship between predictors and the target variable\" could be more specific, such as \"linear relationship between square footage and housing price\".\n",
            "3. The answer does not provide an example of what kind of results the researcher might see, such as a statistical significance value or a confidence interval.\n",
            "\n",
            "**Improvements:**\n",
            "\n",
            "1. Provide more context about why regression analysis is suitable for this problem. For example, you could discuss how it allows for the control of other factors that might affect housing prices.\n",
            "2. Use more technical language to describe the regression analysis. For example, you could discuss the types of coefficients (e.g. slope, intercept) and how they relate to the predicted outcome.\n",
            "3. Provide an example of what kind of results the researcher might see, such as a statistical significance value or a confidence interval. This would help to give a clearer idea of what the results mean.\n",
            "\n",
            "Here is an example of how the answer could be improved:\n",
            "\n",
            "\"Regression analysis can be a powerful tool for predicting housing prices based on various factors, such as square footage and location. By using regression analysis, we can control for other factors that might affect housing prices, such as proximity to public transportation or the quality of local schools. The results of the regression analysis can be interpreted by examining the coefficients, which represent the change in the predicted housing price for a one-unit change in the predictor variable, while holding all other factors constant. For example, if the coefficient for square footage is 0.05, this means that a 100 square foot increase in square footage would result in a 5% increase in the predicted housing price, assuming all other factors remain constant. The statistical significance of the regression analysis and the confidence intervals around the predicted values can also provide valuable insights into the reliability of the model.\"\n"
          ]
        }
      ]
    }
  ]
}